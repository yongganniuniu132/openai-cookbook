{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assistants API Overview\n",
    "\n",
    "The new [Assistants API](https://platform.openai.com/docs/assistants/overview) is a stateful evoltuion of our [Chat Completions API](https://platform.openai.com/docs/guides/text-generation/chat-completions-api) meant to simplify the creation of assistant-like experiences, and enable developer access to powerful tools like Code Interpreter and Knowledge Retreival.\n",
    "\n",
    "## Chat Completions API vs Assistants API\n",
    "\n",
    "The primitives of the **Chat Completions API** are `Messages`, on which you perform on `Completion` with a `Model` (`gpt-3.5-turbo`, `gpt-4`, etc). It is lightweight and powerful, but inherently stateless, which means you have to manage conversation state, tool definitions, retrieval documents, and code execution manually.\n",
    "\n",
    "The primitives of the Assistants API are\n",
    "- `Assistants`, which encapsulate instructions, tools, and (context) documents,\n",
    "- `Threads`, which represent the state of a conversation, and\n",
    "- `Runs`, which power the execution of an `Assistant` on a `Thread`, including multi-step tool use and regular responses.\n",
    "\n",
    "We'll take a look at how these can be used to create powerful, stateful experiences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python SDK Setup\n",
    "\n",
    "> **Note**\n",
    "> We've updated our Python SDK to add support for the Assistants API functions, so you'll need to update it to the latest version (`1.1.1` at time of writing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure it's up to date by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show openai | grep Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Example with Assistants API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assistants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The easiest way to get started with the Assistants API is through the [Assistants Playground](https://platform.openai.com/playground)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"1728\" alt=\"Screenshot 2023-11-08 at 3 28 51 PM\" src=\"https://github.com/ibigio/shell-ai/assets/25421602/11aafc0c-6a03-4dd3-a8f9-ee2f749c30c8\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating an assistant! We'll create a Math Tutor just like in our [docs](https://platform.openai.com/docs/assistants/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"1728\" alt=\"Screenshot 2023-11-08 at 3 31 54 PM\" src=\"https://github.com/ibigio/shell-ai/assets/25421602/f4012fde-875b-42b8-bfe1-21be189d7cfc\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view Assistants you've created in the [Assistants Dashboard](https://platform.openai.com/assistants)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"1728\" alt=\"Screenshot 2023-11-08 at 3 35 15 PM\" src=\"https://github.com/ibigio/shell-ai/assets/25421602/2f0ac3c1-0e1a-4e12-8697-fa2047f9c740\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also create Assistants directly through the Assistants API, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'asst_FzJSwsfIcBfSWWm8rSCwReI5',\n",
       " 'created_at': 1699478811,\n",
       " 'description': None,\n",
       " 'file_ids': [],\n",
       " 'instructions': 'You are a personal math tutor. Answer questions in a sentence or less.',\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-1106-preview',\n",
       " 'name': 'Math Tutor',\n",
       " 'object': 'assistant',\n",
       " 'tools': []}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Just a helper for more readable output\n",
    "def show_json(obj):\n",
    "    return json.loads(obj.json())\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Math Tutor\",\n",
    "    instructions=\"You are a personal math tutor. Answer questions in a sentence or less.\",\n",
    "    model=\"gpt-4-1106-preview\",\n",
    ")\n",
    "show_json(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of whether you create your Assistant through the Dashboard or with the API, you'll want to keep track of the Assistant ID. This is how you'll refer to your Assistant throughout Threads and Runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a new Thread and add a message to it. This will hold the state of our conversation, so we don't have re-send the entire message history each time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'thread_HF1YO13gNDrN5rA0vxJ8txSN',\n",
       " 'created_at': 1699478814,\n",
       " 'metadata': {},\n",
       " 'object': 'thread'}"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thread = client.beta.threads.create()\n",
    "show_json(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_6DrRvEvKfoKDI2J3uOHpUP0x',\n",
       " 'assistant_id': None,\n",
       " 'content': [{'text': {'annotations': [],\n",
       "    'value': 'I need to solve the equation `3x + 11 = 14`. Can you help me?'},\n",
       "   'type': 'text'}],\n",
       " 'created_at': 1699478817,\n",
       " 'file_ids': [],\n",
       " 'metadata': {},\n",
       " 'object': 'thread.message',\n",
       " 'role': 'user',\n",
       " 'run_id': None,\n",
       " 'thread_id': 'thread_HF1YO13gNDrN5rA0vxJ8txSN'}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
    ")\n",
    "show_json(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "> Even though you're only sending new messages, you will still be charged for the tokens of the entire conversation history with each `Run`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runs\n",
    "\n",
    "Notice how the Thread we created is **not** associated with the Assistant we just created! Threads exist independently from Assistants, which may be different from what you'd expect if you've used ChatGPT (where a thread is tied to a model/GPT).\n",
    "\n",
    "To get a completion from an Assistant for a given Thread, we must create a Run. Creating a Run will indicate to an Assistant it should look at the messages in the Thread and take action: either by adding a single response, or using tools. \n",
    "\n",
    "> **Note**\n",
    "> Runs are key difference between the Assistants API and Chat Completions API! While in Chat Completions the model will only ever respond with a single message, in the Assistants API a Run may result in an Assistant using one or multiple tools, and potentially adding multiple messages to the Thread.\n",
    "\n",
    "To get our Assistant to respond to the user, let's create the Run. As mentioned earlier, you must specify _both_ the Assistant _and_ the Thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'run_Jc417uexRM9o0EMUT6A6pehG',\n",
       " 'assistant_id': 'asst_FzJSwsfIcBfSWWm8rSCwReI5',\n",
       " 'cancelled_at': None,\n",
       " 'completed_at': None,\n",
       " 'created_at': 1699478821,\n",
       " 'expires_at': 1699479421,\n",
       " 'failed_at': None,\n",
       " 'file_ids': [],\n",
       " 'instructions': 'You are a personal math tutor. Answer questions in a sentence or less.',\n",
       " 'last_error': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4-1106-preview',\n",
       " 'object': 'thread.run',\n",
       " 'required_action': None,\n",
       " 'started_at': None,\n",
       " 'status': 'queued',\n",
       " 'thread_id': 'thread_HF1YO13gNDrN5rA0vxJ8txSN',\n",
       " 'tools': []}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    ")\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike creating a completion in Chat Completions, creating a Run is an asynchronous operation. It will return immediately with the Run's metadata, which includes a `status` that will initially be set to `queued`. The `status` will be updated as the Assistant performs operations (like using tools and adding messages).\n",
    "\n",
    "To know when the Assistant has completed processing, we can poll the Run in a loop. (We will soon be adding support for streaming.) While we here are only checking for `completed` and `finished`, in practice a `Run` may undergo a variety of status changes which you can choose to surface to the user. (These are called `Steps`, and will be covered later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_on_run(run):\n",
    "    while not (run.status == \"completed\" or run.status == \"failed\"):\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wait_on_run(run)\n",
    "show_json(run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Run has completed we list the Messages in the Thread to see what got added by the Assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "show_json(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Messages are ordered in reverse-chronological order – this was done so the results are always on the first `page` (since results can be paginated). Do keep a look out for this, since this is opposite to messages in the Chat Completions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's ask our Assistant to explain the result a bit further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a message to append to our thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Could you explain this to me?\"\n",
    ")\n",
    "\n",
    "# Execute our run\n",
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "wait_on_run(run)\n",
    "\n",
    "# Get all the messages added after our last user message\n",
    "messages = client.beta.threads.messages.list(thread_id=thread.id, order='asc', after=message.id)\n",
    "show_json(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may feel like a lot of steps to get a response back, especially for this simple example. However, you'll soon see how we can add very powerful functionality to our Assistant without changing much code at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how we could potentially put all of this together. Below is all the code you need to use an Assistant you've created.\n",
    "\n",
    "TODO: free concurrency!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "MATH_ASSISTANT_ID = \"asst_ooPQXw1Jqx0DMH96PEmdvXrw\"\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def get_response(thread):\n",
    "    messages = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id, order=\"asc\"\n",
    "    )\n",
    "    return [m.content[0].text.value for m in messages.data]\n",
    "\n",
    "def submit_message(assistant_id, thread, user_message):\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id, role=\"user\", content=user_message\n",
    "    )\n",
    "    run = client.beta.threads.runs.create(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant_id,\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_thread_and_run(user_input):\n",
    "    thread = client.beta.threads.create()\n",
    "    run = submit_message(MATH_ASSISTANT_ID, thread, user_input)\n",
    "    return thread, run\n",
    "\n",
    "# Emulating concurrent user requests\n",
    "thread1, run1 = create_thread_and_run(\"I need to solve the equation `3x + 11 = 14`. Can you help me?\")\n",
    "thread2, run2 = create_thread_and_run(\"Could you explain linear algebra to me?\")\n",
    "thread3, run3 = create_thread_and_run(\"I don't like math. What can I do?\")\n",
    "\n",
    "# Now all Runs are executing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I need to solve the equation `3x + 11 = 14`. Can you help me?', 'Yes, first subtract 11 from both sides to get `3x = 3`, then divide both sides by 3 to get `x = 1`.']\n",
      "['Could you explain linear algebra to me?', 'Linear algebra is the branch of mathematics concerning linear equations, linear functions, and their representations through matrices and vector spaces.']\n",
      "[\"I don't like math. What can I do?\", 'Explore different ways of learning math, such as through games, apps, or relating it to your interests, and consider a personal tutor who can tailor lessons to your learning style.']\n"
     ]
    }
   ],
   "source": [
    "def wait_on_run(thread, run):\n",
    "    while not (run.status == \"completed\" or run.status == \"failed\"):\n",
    "        run = client.beta.threads.runs.retrieve(\n",
    "            thread_id=thread.id,\n",
    "            run_id=run.id,\n",
    "        )\n",
    "    return run\n",
    "\n",
    "# Wait for Run 1\n",
    "run1 = wait_on_run(thread1, run1)\n",
    "print(get_response(thread1))\n",
    "\n",
    "# Wait for Run 2\n",
    "run2 = wait_on_run(thread2, run2)\n",
    "print(get_response(thread2))\n",
    "\n",
    "# Wait for Run 3\n",
    "run3 = wait_on_run(thread3, run3)\n",
    "print(get_response(thread3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you may have noticed is that this code is actually not specific to our math Assistant at all... this code will work for any new Assistant you create! This is the power of the Assistants API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Interpreter\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
